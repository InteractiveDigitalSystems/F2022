{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "burning-estimate",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IDS #8 - Training and using Machine Learning models for Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-difficulty",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Supervised Learning recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-uzbekistan",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training and inference\n",
    "- Training can take a lot of time and ressources.\n",
    "- Separation between training models and inference (making new predictions).\n",
    "- We therefore need to save models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f32ea4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is a model?\n",
    "* Many definitions:\n",
    "> The term ML model refers to the model artifact that is created by the training process. \n",
    ">\n",
    "> <cite>[aws](https://docs.aws.amazon.com/machine-learning/latest/dg/training-ml-models.html)</cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6bdaa9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Weights and biases (linear function)\n",
    "$\\large y=mx+b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77ce4e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Weights and biases (neural networks)\n",
    "![image](images/nn_weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660996c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Models can be very valuable!\n",
    "*  OpenAI's GPT model architecture is up for everybody, while the trained model was not released for various security reasons.\n",
    "* The full trained model might be of very high value!\n",
    "* Architectures are often shared through research, while trained models aren't.\n",
    "* Lack of shared models are can lead to a reproducibility crisis in research [[1]](https://www.nature.com/articles/d41586-019-03895-5) [[2]](https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/) [[3]](https://www.technologyreview.com/2020/11/12/1011944/artificial-intelligence-replication-crisis-science-big-tech-google-deepmind-facebook-openai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc7239",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Price estimates:\n",
    "* MobileNet on AWS (10,000 DKK)\n",
    "* GPT-3 (12,000,000 USD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeaa0b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Finding ML models\n",
    "* [Model Zoo](https://modelzoo.co/)\n",
    "* [TensorFlow Hub](https://tfhub.dev/)\n",
    "* [TensorFlow model garden](https://github.com/tensorflow/models/tree/master/official)\n",
    "* [MediaPipe](https://google.github.io/mediapipe/) (Really nice and intuitive w/ Python demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32e8a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pickle\n",
    "![alt text](images/pickle.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b3c3e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Pickle is used for serializing and de-serializing Python object structures, also called marshalling or flattening. Serialization refers to the process of converting an object in memory to a byte stream that can be stored on disk or sent over a network. Later on, this character stream can then be retrieved and de-serialized back to a Python object.\n",
    ">\n",
    "> -- [DataCamp](https://www.datacamp.com/community/tutorials/pickle-python-tutorial#what)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c9d94",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Pickling can be used for saving objects to disk. Can be useful for saving trained machine learning algorithms, complex data structures and dictionaries, lists and so.\n",
    "\n",
    "Don't use pickling across various programming languages. Only unpickle data from a trusted source!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a533245",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pickle vs JSON\n",
    "* JSON is easily read by humans and is agnostic of programming languages.\n",
    "* Pickled data cannot be read by humans and works only in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756aa31a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cd970",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "def some_special_function():\n",
    "    print('something special')\n",
    "    \n",
    "image = Image.open('images/ruc_logo.png')\n",
    "\n",
    "data = {\n",
    "  \"year\": 1964,\n",
    "  \"function\": some_special_function, # we can save functions\n",
    "  \"image\": image # or complex data such as images!\n",
    "}\n",
    "\n",
    "filename = 'data.pkl'\n",
    "outfile = open(filename,'wb') # 'wb' means write binary\n",
    "pickle.dump(data,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0d524",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# reading from pickle file\n",
    "infile = open(filename,'rb')\n",
    "data = pickle.load(infile)\n",
    "infile.close()\n",
    "data['function']()\n",
    "data['image'] # im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-spell",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Common formats for saving ML models\n",
    "- SavedModel (Tensorflow)\n",
    "- HDF5 (Tensorflow)\n",
    "- TFJS (Tensorflow.js)\n",
    "- state_dict (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-attention",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Teachable Machines!\n",
    "- Let's get on with it and try to teach our own image recognition model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-encyclopedia",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Demo](https://teachablemachine.withgoogle.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-master",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Teachable Machines exercise start\n",
    "* Train an image recognition model with 3+ classes using [Teachable Machine](https://teachablemachine.withgoogle.com/)\n",
    "* Explore it's stability!\n",
    "    * How does it handle different kinds of lighting, rotation, scaling?\n",
    "    * Can you make it recognise different objects as the same class? (e.g. recognising different kinds of cups, pencils, hats, etc..)\n",
    "* Download the model and run the [tm_webcam](https://github.com/InteractiveDigitalSystems/F2022/blob/main/8%20-%20Machine%20Learning%20II%20-%20Training%20and%20using%20Machine%20Learning%20models/teachable_machines/tm_webcam.py) Python script.\n",
    "    * Examine the script! Adjust the appropriate labels in line 24 and print the outputs.\n",
    "    * Can you get it to print the prediction it has least confidence in?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-minute",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does Teachable Machines work?\n",
    "* Under the hood we have a CNN (Convolutional Neural Network)\n",
    "* CNNs are widely used in Natural Language Processing, image segmentation and image recognition.\n",
    "* It gradually creates features maps that are shift variant and are fed into the following layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0f362",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image](images/feature_map.png)\n",
    "From [Feature Map](https://medium.com/@chriskevin_80184/feature-maps-ee8e11a71f9e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0dd247",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What's the output of a neural network doing object recognition?\n",
    "* Code example (Teachable Machine)\n",
    "* Input image\n",
    "* Output vector\n",
    "    * arg-maxing output to find label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeabfc8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transfer learning\n",
    "Transfer learning is taking a pre-trained (generic) neural network and training it on a specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-witch",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![image](images/transfer_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942f0f4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why transfer learn?\n",
    "* Saves time\n",
    "* Needing less data\n",
    "* Often performing better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-pillow",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tips:\n",
    "* If the new dataset is very small, itâ€™s better to train only the final layers of the network to avoid overfitting, keeping all other layers fixed. So remove the final layers of the pre-trained network. Add new layers . Retrain only the new layers.\n",
    "* If the new dataset is very much large, retrain the whole network with initial weights from the pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-bradford",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transfer learning and teachable machines\n",
    "* Image recognition built on pre-trained mobilenet\n",
    "* Audio recognition built on pre-trained [Speech Command Recognizer](https://github.com/tensorflow/tfjs-models/tree/master/speech-commands)\n",
    "\n",
    "\n",
    "_Source: [TM FAQ](https://teachablemachine.withgoogle.com/faq) </cite>_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb86e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pose estimation\n",
    "![image](https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/dance_foot.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481f551",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Pose estimation is the task of estimation the pose of a person\n",
    "* Input video (image) output is spatial locations of key body joints (landsmarks or keypoints)\n",
    "* Became well-known through [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose)\n",
    "* Only recently became more accessible through e.g. [MediaPipe](https://mediapipe.dev/), [moveNet](https://www.tensorflow.org/hub/tutorials/movenet) and [PoseNet](https://github.com/tensorflow/tfjs-models/tree/master/posenet) (all tensorflow)\n",
    "* Models vary: some favour stability and accuracy over speed, some have fewer landmarks and others are focused only on e.g. hands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0154fde3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hasn't really seen wide use interaction yet. Reasons? What kind of usage could there be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95aecf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Output of a pose estimation model\n",
    "- Let's find out! - [example](https://editor.p5js.org/ima_ml/sketches/lrBwwxGiF) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f49e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pose estimation exercise\n",
    "[p5js link](https://editor.p5js.org/frederiktj/sketches/-Qb_ws-nZ)\n",
    "\n",
    "* Disable click\n",
    "* Replace mouse coordinates which decided finger coordinates\n",
    "* Create isPressed function with hands! (could e.g. be using [dist](https://p5js.org/reference/#/p5/dist))\n",
    "* Reenable click but without mouse\n",
    "* Polish code! Make modular and usable. Can we create a hand class that has some of the same functions we expect from a mouse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af27dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
